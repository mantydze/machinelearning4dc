{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import setGPU\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from sklearn.utils import shuffle\n",
    "import h5py\n",
    "\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "LABELS = [\"Normal\", \"Anomalous\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, scale, RobustScaler, normalize, MaxAbsScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature names\n",
    "var_names_reduced5 = ['qPFJetPt', 'qPFJetEta', 'qPFJetPhi', 'qPFJet0Pt', 'qPFJet1Pt', 'qPFJet2Pt', 'qPFJet3Pt', 'qPFJet4Pt', 'qPFJet5Pt', 'qPFJet0Eta', 'qPFJet1Eta', 'qPFJet2Eta', 'qPFJet3Eta', 'qPFJet4Eta', 'qPFJet5Eta', 'qPFJet0Phi', 'qPFJet1Phi', 'qPFJet2Phi', 'qPFJet3Phi', 'qPFJet4Phi', 'qPFJet5Phi', 'qPFJet4CHS0Pt', 'qPFJet4CHS1Pt', 'qPFJet4CHS2Pt', 'qPFJet4CHS3Pt', 'qPFJet4CHS4Pt', 'qPFJet4CHS5Pt', 'qPFJet4CHS0Eta', 'qPFJet4CHS1Eta', 'qPFJet4CHS2Eta', 'qPFJet4CHS3Eta', 'qPFJet4CHS4Eta', 'qPFJet4CHS5Eta', 'qPFJet4CHS0Phi', 'qPFJet4CHS1Phi', 'qPFJet4CHS2Phi', 'qPFJet4CHS3Phi', 'qPFJet4CHS4Phi', 'qPFJet4CHS5Phi', 'qPFJet8CHS0Pt', 'qPFJet8CHS1Pt', 'qPFJet8CHS2Pt', 'qPFJet8CHS3Pt', 'qPFJet8CHS4Pt', 'qPFJet8CHS5Pt', 'qPFJet8CHS0Eta', 'qPFJet8CHS1Eta', 'qPFJet8CHS2Eta', 'qPFJet8CHS3Eta', 'qPFJet8CHS4Eta', 'qPFJet8CHS5Eta', 'qPFJet8CHS0Phi', 'qPFJet8CHS1Phi', 'qPFJet8CHS2Phi', 'qPFJet8CHS3Phi', 'qPFJet8CHS4Phi', 'qPFJet8CHS5Phi', 'qPFJetEI0Pt', 'qPFJetEI1Pt', 'qPFJetEI2Pt', 'qPFJetEI3Pt', 'qPFJetEI4Pt', 'qPFJetEI5Pt', 'qPFJetEI0Eta', 'qPFJetEI1Eta', 'qPFJetEI2Eta', 'qPFJetEI3Eta', 'qPFJetEI4Eta', 'qPFJetEI5Eta', 'qPFJetEI0Phi', 'qPFJetEI1Phi', 'qPFJetEI2Phi', 'qPFJetEI3Phi', 'qPFJetEI4Phi', 'qPFJetEI5Phi', 'qPFJet8CHSSD0Pt', 'qPFJet8CHSSD1Pt', 'qPFJet8CHSSD2Pt', 'qPFJet8CHSSD3Pt', 'qPFJet8CHSSD4Pt', 'qPFJet8CHSSD5Pt', 'qPFJet8CHSSD0Eta', 'qPFJet8CHSSD1Eta', 'qPFJet8CHSSD2Eta', 'qPFJet8CHSSD3Eta', 'qPFJet8CHSSD4Eta', 'qPFJet8CHSSD5Eta', 'qPFJet8CHSSD0Phi', 'qPFJet8CHSSD1Phi', 'qPFJet8CHSSD2Phi', 'qPFJet8CHSSD3Phi', 'qPFJet8CHSSD4Phi', 'qPFJet8CHSSD5Phi', 'qPFJetTopCHS0Pt', 'qPFJetTopCHS1Pt', 'qPFJetTopCHS2Pt', 'qPFJetTopCHS3Pt', 'qPFJetTopCHS4Pt', 'qPFJetTopCHS5Pt', 'qPFJetTopCHS0Eta', 'qPFJetTopCHS1Eta', 'qPFJetTopCHS2Eta', 'qPFJetTopCHS3Eta', 'qPFJetTopCHS4Eta', 'qPFJetTopCHS5Eta', 'qPFJetTopCHS0Phi', 'qPFJetTopCHS1Phi', 'qPFJetTopCHS2Phi', 'qPFJetTopCHS3Phi', 'qPFJetTopCHS4Phi', 'qPFJetTopCHS5Phi', 'qCalJet0Pt', 'qCalJet1Pt', 'qCalJet2Pt', 'qCalJet3Pt', 'qCalJet4Pt', 'qCalJet5Pt', 'qCalJet0Eta', 'qCalJet1Eta', 'qCalJet2Eta', 'qCalJet3Eta', 'qCalJet4Eta', 'qCalJet5Eta', 'qCalJet0Phi', 'qCalJet1Phi', 'qCalJet2Phi', 'qCalJet3Phi', 'qCalJet4Phi', 'qCalJet5Phi', 'qCalJet0En', 'qCalJet1En', 'qCalJet2En', 'qCalJet3En', 'qCalJet4En', 'qCalJet5En', 'qPho0Pt', 'qPho1Pt', 'qPho2Pt', 'qPho3Pt', 'qPho4Pt', 'qPho5Pt', 'qPho0Eta', 'qPho1Eta', 'qPho2Eta', 'qPho3Eta', 'qPho4Eta', 'qPho5Eta', 'qPho0Phi', 'qPho1Phi', 'qPho2Phi', 'qPho3Phi', 'qPho4Phi', 'qPho5Phi', 'qPho0En', 'qPho1En', 'qPho2En', 'qPho3En', 'qPho4En', 'qPho5En', 'qgedPho0Pt', 'qgedPho1Pt', 'qgedPho2Pt', 'qgedPho3Pt', 'qgedPho4Pt', 'qgedPho5Pt', 'qgedPho0Eta', 'qgedPho1Eta', 'qgedPho2Eta', 'qgedPho3Eta', 'qgedPho4Eta', 'qgedPho5Eta', 'qgedPho0Phi', 'qgedPho1Phi', 'qgedPho2Phi', 'qgedPho3Phi', 'qgedPho4Phi', 'qgedPho5Phi', 'qgedPho0En', 'qgedPho1En', 'qgedPho2En', 'qgedPho3En', 'qgedPho4En', 'qgedPho5En', 'qMu0Pt', 'qMu1Pt', 'qMu2Pt', 'qMu3Pt', 'qMu4Pt', 'qMu5Pt', 'qMu0Eta', 'qMu1Eta', 'qMu2Eta', 'qMu3Eta', 'qMu4Eta', 'qMu5Eta', 'qMu0Phi', 'qMu1Phi', 'qMu2Phi', 'qMu3Phi', 'qMu4Phi', 'qMu5Phi', 'qMu0En', 'qMu1En', 'qMu2En', 'qMu3En', 'qMu4En', 'qMu5En', 'qMuCosm0Pt', 'qMuCosm1Pt', 'qMuCosm2Pt', 'qMuCosm3Pt', 'qMuCosm4Pt', 'qMuCosm5Pt', 'qMuCosm0Eta', 'qMuCosm1Eta', 'qMuCosm2Eta', 'qMuCosm3Eta', 'qMuCosm4Eta', 'qMuCosm5Eta', 'qMuCosm0Phi', 'qMuCosm1Phi', 'qMuCosm2Phi', 'qMuCosm3Phi', 'qMuCosm4Phi', 'qMuCosm5Phi', 'qMuCosm0En', 'qMuCosm1En', 'qMuCosm2En', 'qMuCosm3En', 'qMuCosm4En', 'qMuCosm5En', 'qMuCosmLeg0Pt', 'qMuCosmLeg1Pt', 'qMuCosmLeg2Pt', 'qMuCosmLeg3Pt', 'qMuCosmLeg4Pt', 'qMuCosmLeg5Pt', 'qMuCosmLeg0Eta', 'qMuCosmLeg1Eta', 'qMuCosmLeg2Eta', 'qMuCosmLeg3Eta', 'qMuCosmLeg4Eta', 'qMuCosmLeg5Eta', 'qMuCosmLeg0Phi', 'qMuCosmLeg1Phi', 'qMuCosmLeg2Phi', 'qMuCosmLeg3Phi', 'qMuCosmLeg4Phi', 'qMuCosmLeg5Phi', 'qMuCosmLeg0En', 'qMuCosmLeg1En', 'qMuCosmLeg2En', 'qMuCosmLeg3En', 'qMuCosmLeg4En', 'qMuCosmLeg5En', 'qPFJet4CHSPt', 'qPFJet4CHSEta', 'qPFJet4CHSPhi', 'qPFJet8CHSPt', 'qPFJet8CHSEta', 'qPFJet8CHSPhi', 'qPFJetEIPt', 'qPFJetEIEta', 'qPFJetEIPhi', 'qPFJet8CHSSDPt', 'qPFJet8CHSSDEta', 'qPFJet8CHSSDPhi', 'qPFJetTopCHSPt', 'qPFJetTopCHSEta', 'qPFJetTopCHSPhi', 'qPFChMetPt', 'qPFChMetPhi', 'qPFMetPt', 'qPFMetPhi', 'qNVtx', 'qCalJetPt', 'qCalJetEta', 'qCalJetPhi', 'qCalJetEn', 'qCalMETPt', 'qCalMETPhi', 'qCalMETEn', 'qCalMETBEPt', 'qCalMETBEPhi', 'qCalMETBEEn', 'qCalMETBEFOPt', 'qCalMETBEFOPhi', 'qCalMETBEFOEn', 'qCalMETMPt', 'qCalMETMPhi', 'qCalMETMEn', 'qSCEn', 'qSCEta', 'qSCPhi', 'qSCEtaWidth', 'qSCPhiWidth', 'qSCEnhfEM', 'qSCEtahfEM', 'qSCPhihfEM', 'qSCEn5x5', 'qSCEta5x5', 'qSCPhi5x5', 'qSCEtaWidth5x5', 'qSCPhiWidth5x5', 'qCCEn', 'qCCEta', 'qCCPhi', 'qCCEn5x5', 'qCCEta5x5', 'qCCPhi5x5', 'qPhoPt', 'qPhoEta', 'qPhoPhi', 'qPhoEn_', 'qPhoe1x5_', 'qPhoe2x5_', 'qPhoe3x3_', 'qPhoe5x5_', 'qPhomaxenxtal_', 'qPhosigmaeta_', 'qPhosigmaIeta_', 'qPhor1x5_', 'qPhor2x5_', 'qPhor9_', 'qgedPhoPt', 'qgedPhoEta', 'qgedPhoPhi', 'qgedPhoEn_', 'qgedPhoe1x5_', 'qgedPhoe2x5_', 'qgedPhoe3x3_', 'qgedPhoe5x5_', 'qgedPhomaxenxtal_', 'qgedPhosigmaeta_', 'qgedPhosigmaIeta_', 'qgedPhor1x5_', 'qgedPhor2x5_', 'qgedPhor9_', 'qMuPt', 'qMuEta', 'qMuPhi', 'qMuEn_', 'qMuCh_', 'qMuChi2_', 'qMuCosmPt', 'qMuCosmEta', 'qMuCosmPhi', 'qMuCosmEn_', 'qMuCosmCh_', 'qMuCosmChi2_', 'qMuCosmLegPt', 'qMuCosmLegEta', 'qMuCosmLegPhi', 'qMuCosmLegEn_', 'qMuCosmLegCh_', 'qMuCosmLegChi2_', 'qSigmaIEta', 'qSigmaIPhi', 'qr9', 'qHadOEm', 'qdrSumPt', 'qdrSumEt', 'qeSCOP', 'qecEn', 'qUNSigmaIEta', 'qUNSigmaIPhi', 'qUNr9', 'qUNHadOEm', 'qUNdrSumPt', 'qUNdrSumEt', 'qUNeSCOP', 'qUNecEn', 'qEBenergy', 'qEBtime', 'qEBchi2', 'qEBiEta', 'qEBiPhi', 'qEEenergy', 'qEEtime', 'qEEchi2', 'qEEix', 'qEEiy', 'qESenergy', 'qEStime', 'qESix', 'qESiy', 'qHBHEenergy', 'qHBHEtime', 'qHBHEauxe', 'qHBHEieta', 'qHBHEiphi', 'qHFenergy', 'qHFtime', 'qHFieta', 'qHFiphi', 'qPreShEn', 'qPreShEta', 'qPreShPhi', 'qPreShYEn', 'qPreShYEta', 'qPreShYPhi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Authenticate in order to get permission for eos\n",
    "os.system(\"echo %s | kinit\" % getpass.getpass())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load h5 files\n",
    "\n",
    "\n",
    "#Choose where to load the files from\n",
    "# b_h5 = '/eos/cms/store/user/fsiroky/hdf5_data/'\n",
    "# b_h5       = '/eos/cms/store/user/fsiroky/lumih5/'\n",
    "b_h5 = '/eos/cms/store/user/fsiroky/consistentlumih5/'   #These files are used for the analysis\n",
    "# b_h5   = '/afs/cern.ch/user/f/fsiroky/public/'\n",
    "# b_h5 = '/mnt/hdf5test/'\n",
    "# b_h5   = '/home/test_local/'\n",
    "\n",
    "pds  = {1: 'BTagCSV', 2: 'BTagMu', 3: 'Charmonium', 4:'DisplacedJet', 5: 'DoubleEG',\n",
    "        6: 'DoubleMuon', 7: 'DoubleMuonLowMass',\n",
    "       # 8: 'FSQJets', 9: 'HighMultiplicityEOF', #NOT ENOUGH DATA, NOTEBOOK FAILES\n",
    "        10: 'HTMHT', 11: 'JetHT', 12: 'MET',\n",
    "       # 13: 'MinimumBias', #NOT ENOUGH DATA\n",
    "        14: 'MuonEG', 15: 'MuOnia',\n",
    "       # 16: 'NoBPTX',\n",
    "        17: 'SingleElectron', 18: 'SingleMuon', 19: 'SinglePhoton', 20: 'Tau', 21: 'ZeroBias'\n",
    "}\n",
    "\n",
    "      \n",
    "def get_jets(bg_files, bg_jets, sig_files, sig_jets):\n",
    "    #Use np.empty([0,2802]) for both good and bad jets, if you use b_h5 = '/eos/cms/store/user/fsiroky/hdf5_data/'\n",
    "    good_jets = np.empty([0,2813])\n",
    "    bad_jets  = np.empty([0,2813])\n",
    "                   # Control which time intervals files per PD to load with range in the for loop\n",
    "    for i in range(0,len(bg_files)):   #0\n",
    "        try:\n",
    "            bg_jetfile  = h5py.File(bg_files[i],'r')\n",
    "            bg_jet      = bg_jetfile[bg_jets[i]][:]\n",
    "            sig_jetfile = h5py.File(sig_files[i],'r')\n",
    "            sig_jet     = sig_jetfile[sig_jets[i]][:]\n",
    "            # print(bad_jets.shape, bg_jet.shape)\n",
    "            bad_jets    = np.concatenate((bad_jets, bg_jet), axis=0)\n",
    "            good_jets = np.concatenate((good_jets, sig_jet), axis=0)\n",
    "            print( \"Number of good lumis: \", len(sig_jet), \" Number of bad lumis: \", len(bg_jet)) \n",
    "\n",
    "        except OSError as error:\n",
    "            print(\"This Primary Dataset doesn't have \", bg_jets[i], error )\n",
    "            continue\n",
    "    return good_jets, bad_jets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good lumis:  17238  Number of bad lumis:  492\n",
      "Number of good lumis:  26782  Number of bad lumis:  121\n",
      "Number of good lumis:  15558  Number of bad lumis:  638\n",
      "Number of good lumis:  17901  Number of bad lumis:  70\n",
      "Number of good lumis:  40180  Number of bad lumis:  201\n",
      "Number of good lumis:  41347  Number of bad lumis:  3156\n"
     ]
    }
   ],
   "source": [
    "#Choose which PD to load\n",
    "nbr = 11 #Jvariable\n",
    "\n",
    "bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "#Load good and bad jets\n",
    "good_jets, bad_jets = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "\n",
    "# #Choose which PD to load\n",
    "# nbr = 3 #Charmonium\n",
    "\n",
    "# bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "#             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "# bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "#             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "# sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "#             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "# sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "#             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "# #Load good and bad jets\n",
    "# good_jets2, bad_jets2 = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "# #Choose which PD to load\n",
    "# nbr = 15 #\n",
    "\n",
    "# bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "#             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "# bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "#             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "# sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "#             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "# sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "#             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "# #Load good and bad jets\n",
    "# good_jets3, bad_jets3 = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "\n",
    "# #Choose which PD to load\n",
    "# nbr = 14\n",
    "\n",
    "# bg_files  = [b_h5+pds[nbr]+'_C_background.h5',b_h5+pds[nbr]+'_D_background.h5', b_h5+pds[nbr]+'_E_background.h5',\n",
    "#             b_h5+pds[nbr]+'_F_background.h5', b_h5+pds[nbr]+'_G_background.h5', b_h5+pds[nbr]+'_H_background.h5']\n",
    "\n",
    "# bg_jets   = [pds[nbr]+\"_C_background\", pds[nbr]+\"_D_background\", pds[nbr]+\"_E_background\",\n",
    "#             pds[nbr]+\"_F_background\", pds[nbr]+\"_G_background\", pds[nbr]+\"_H_background\"]\n",
    "\n",
    "# sig_files = [b_h5+pds[nbr]+'_C_signal.h5',b_h5+pds[nbr]+'_D_signal.h5', b_h5+pds[nbr]+'_E_signal.h5',\n",
    "#             b_h5+pds[nbr]+'_F_signal.h5', b_h5+pds[nbr]+'_G_signal.h5', b_h5+pds[nbr]+'_H_signal.h5']\n",
    "\n",
    "# sig_jets  = [pds[nbr]+\"_C_signal\", pds[nbr]+\"_D_signal\", pds[nbr]+\"_E_signal\",\n",
    "#             pds[nbr]+\"_F_signal\", pds[nbr]+\"_G_signal\", pds[nbr]+\"_H_signal\"]\n",
    "\n",
    "# #Load good and bad jets\n",
    "# good_jets4, bad_jets4 = get_jets(bg_files, bg_jets, sig_files, sig_jets)\n",
    "\n",
    "\n",
    "\n",
    "#Assign good jets class label 0\n",
    "df1 = pd.DataFrame(good_jets)\n",
    "# cutted_df = df1.iloc[0:25000, :]   #Temporarily to make training faster\n",
    "# df1 = cutted_df                   #Temporarily to make training faster\n",
    "df1['class'] = 0\n",
    "\n",
    "#Assign bad_jets class label  1\n",
    "df2 = pd.DataFrame(bad_jets)\n",
    "# cutted_df = df2.iloc[0:, :]    #Temporarily to make training faster\n",
    "# df2 = cutted_df                   #Temporarily to make training faster\n",
    "df2['class'] = 1\n",
    "\n",
    "# #Assign good jets class label 0\n",
    "# df3 = pd.DataFrame(good_jets2)\n",
    "\n",
    "# df3['class'] = 0\n",
    "\n",
    "# #Assign bad_jets class label  1\n",
    "# df4 = pd.DataFrame(bad_jets2)\n",
    "\n",
    "# df4['class'] = 1\n",
    "\n",
    "\n",
    "# #Assign good jets class label 0\n",
    "# df5 = pd.DataFrame(good_jets3)\n",
    "\n",
    "# df5['class'] = 0\n",
    "\n",
    "# #Assign bad_jets class label  1\n",
    "# df6 = pd.DataFrame(bad_jets3)\n",
    "\n",
    "# df6['class'] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df7 = pd.DataFrame(good_jets4)\n",
    "# df7['class'] = 0\n",
    "\n",
    "# df8 = pd.DataFrame(bad_jets4)\n",
    "# df8['class'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# del(good_jets)\n",
    "# del(bad_jets)\n",
    "#Concatenate them\n",
    "frames  = [df1,df2] \n",
    "#frames = [df1,df2,df3,df4,df5,df6]   #Use something like this if you want to load multiple PDs\n",
    "# frames = [df1,df2,df3,df4,df5,df6,df7,df8]\n",
    "data   = pd.concat(frames)\n",
    "del(frames)\n",
    "# del(df1)\n",
    "# del(df2)\n",
    "\n",
    "data.drop(2812, axis=1, inplace=True) #Drop per_pd flags\n",
    "\n",
    "#The +7 every\n",
    "data = data.sort_values([2807,2808], ascending=[True,True]) #Sort by runID and then by lumiID\n",
    "data = data.reset_index(drop=True)  #Reset index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = data.reindex(index=range(0,len(data)))\n",
    "#Shuffle them randomly\n",
    "# data = shuffle(data)\n",
    "# data = data.reset_index(drop=True)\n",
    "\n",
    "#Save labels and delete them from df not to cheat during training\n",
    "# labels = data['class'].astype(int)\n",
    "# del data['class']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Relabelling incorrect \"Fede json\" with updated one by current choice\n",
    "\n",
    "def json_checker(json_file, orig_runid, orig_lumid): #lookup for labels and appends\n",
    "    outcome = 5  #Should not be anywhere\n",
    "    for k,v in json_file.items():\n",
    "        if (int(k) == orig_runid):\n",
    "            for d in v: #Checks each inner loop of the json per runID\n",
    "                for i in range (d[0], d[1]+1):\n",
    "#                     print(\"key of json is \", k, \" value of json is \", v)\n",
    "# #                     print(v[0][0], \"and\", v[0][1])\n",
    "#                     print(\"current inner list is\", d, \"and range is\", d[0], \" to \", d[1])\n",
    "#                     print(\"i is \", i)\n",
    "                    if i == orig_lumid:\n",
    "#                         print(\"Flagging as bad\")\n",
    "                        outcome =0  #0 means good lumi! (to be compatible with code anomaly_detection.ipynb[mse ae])\n",
    "                        return(outcome)\n",
    "                \n",
    "            \n",
    "        \n",
    "    outcome = 1 #1 means bad lumisection! (to be compatible with code anomaly_detection.ipynb [mse autoencoder])\n",
    "    return(outcome)\n",
    "\n",
    "#Contains golden json\n",
    "json_file_path = '/afs/cern.ch/user/f/fsiroky/public/Cert_271036-284044_13TeV_PromptReco_Collisions16_JSON.txt'\n",
    "\n",
    "def add_flags_from_json(output_json, data):\n",
    "    output_json = json.load(open(json_file_path))\n",
    "    new_json_class = np.empty([data.shape[0],1])\n",
    "    for i in range(0, data.shape[0]):\n",
    "        orig_runid = data[2807][i]\n",
    "        orig_runid = int(orig_runid)\n",
    "        orig_lumid = data[2808][i]\n",
    "        orig_lumid = int(orig_lumid)\n",
    "        new_json_class[i,0] = int(json_checker(output_json, orig_runid, orig_lumid))\n",
    "    data['preco_json'] = new_json_class #PromptReco GOLDEN json\n",
    "    return data\n",
    "\n",
    "new_data = add_flags_from_json(json_file_path, data)\n",
    " \n",
    "del(new_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO!\n",
    "#Check how many good lumis and anomalous ones we have\n",
    "\n",
    "# print(\"Laaalelaaa\", data)\n",
    "\n",
    "# anomalies = data[data['class'] == 1]\n",
    "# normal    = data[data['class'] == 0]\n",
    "\n",
    "# print(\"Number of anomalies: \", anomalies.shape)\n",
    "# del(anomalies)\n",
    "\n",
    "# print(\"Number of normals: \", normal.shape)\n",
    "# del(normal)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save of RunIDs and LumiIDs done\n",
      "RunID and LumiID dropped\n"
     ]
    }
   ],
   "source": [
    "#Save runIDs and lumiIDs and instantaneous luminosities for later, because now we drop them before training\n",
    "\n",
    "runIDs  = data[2807].astype(int)\n",
    "lumiIDs = data[2808].astype(int)\n",
    "lumisections = data[2809].astype(float)  #lumisections means inst. luminosities -  CHANGE!\n",
    "\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/datarunIDs.npy',  runIDs)\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/datalumiIDs.npy', lumiIDs)\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/lumisections.npy', lumisections)\n",
    "\n",
    "\n",
    "print(\"Save of RunIDs and LumiIDs done\")\n",
    "\n",
    "# print(data)\n",
    "data.drop(2800+7, axis=1, inplace=True) #drop RunID before normalizing and training\n",
    "data.drop(2801+7, axis=1, inplace=True) #drop LumiID before normalizing and training\n",
    "print(\"RunID and LumiID dropped\")\n",
    "# print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ORIGINAL ONE\n",
    "#Normalize the data to make training better\n",
    "\n",
    "cutted_data = data.iloc[:, 0:2810]\n",
    "#classes     = data.iloc[:, 2805:2806] \n",
    "classes      = data.iloc[:,-1] #Take PromptReco json\n",
    "\n",
    "\n",
    "# print(classes.shape)\n",
    "np_scaled = StandardScaler().fit_transform(cutted_data.values)\n",
    "# np_scaled = MaxAbsScaler().fit_transform(np_scaled) \n",
    "\n",
    "# print(\"1111\",np_scaled)\n",
    "\n",
    "\n",
    "# np_scaled = scale(cutted_data, axis = 1, with_mean=True, with_std=True, copy=True)\n",
    "datas = pd.DataFrame(np_scaled)\n",
    "# datas = pd.DataFrame(np_scaled, index=cutted_data.index, columns=cutted_data.columns)\n",
    "\n",
    "# print(\"2222\",datas)\n",
    "\n",
    "# del(np_scaled)\n",
    "del(cutted_data)\n",
    "# print(\"Datas first: \", datas)\n",
    "datas[2810] = runIDs   #Append runID back after scaling\n",
    "datas[2811] = lumiIDs  #Append lumiID back after scaling\n",
    "datas['qlabel'] = classes  #qlabel is goldenJSON now\n",
    "\n",
    "# print(\"After scale\", datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Normalize the data to make training better\n",
    "\n",
    "# cutted_data = data.iloc[:, 0:2803+7]\n",
    "# #classes     = data.iloc[:, 2805:2806] \n",
    "# classes      = data.iloc[:,-1] #Take PromptReco json\n",
    "\n",
    "\n",
    "# # print(classes.shape)\n",
    "# np_scaled = StandardScaler().fit_transform(cutted_data.values)\n",
    "# # np_scaled = MaxAbsScaler().fit_transform(np_scaled) \n",
    "\n",
    "# # print(\"1111\",np_scaled)\n",
    "\n",
    "\n",
    "# # np_scaled = scale(cutted_data, axis = 1, with_mean=True, with_std=True, copy=True)\n",
    "# datas = pd.DataFrame(np_scaled)\n",
    "# # datas = pd.DataFrame(np_scaled, index=cutted_data.index, columns=cutted_data.columns)\n",
    "\n",
    "# # print(\"2222\",datas)\n",
    "\n",
    "# # del(np_scaled)\n",
    "# del(cutted_data)\n",
    "# # print(\"Datas first: \", datas)\n",
    "# datas[2803+7] = runIDs   #Append runID back after scaling\n",
    "# datas[2804+7] = lumiIDs  #Append lumiID back after scaling\n",
    "# datas['qlabel'] = classes  #qlabel is goldenJSON now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163684, 2813)\n"
     ]
    }
   ],
   "source": [
    "#ORIGINAL ONE\n",
    "#TEST/TRAIN SPLIT\n",
    "\n",
    "# X_train, X_test = train_test_split(datas, test_size=0.15, random_state=RANDOM_SEED) # This works when we split rndmly\n",
    "split_nbr = round(datas.shape[0]*0.20)  #0.10 means 10% to the validation set\n",
    "\n",
    "print(datas.shape)\n",
    "X_train = datas.iloc[0:(datas.shape[0] - split_nbr) ,:]\n",
    "X_test  = datas.iloc[(datas.shape[0] - split_nbr): (datas.shape[0]) ,:]\n",
    "last_train_idx = X_train.shape[0]\n",
    "\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/last_train_idx.npy', last_train_idx)\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "\n",
    "del(datas)\n",
    "X_train = X_train[X_train['qlabel']== 0]\n",
    "# print(X_train)\n",
    "X_train = X_train.drop(['qlabel'], axis=1)\n",
    "\n",
    "ae_lumis = X_train[2807].astype(float)\n",
    "# print(\"ae lumis\", ae_lumis, \"ae_lumis shape\", ae_lumis.shape)\n",
    "# print(\"XTEEEEST before PerPD json beginn\")\n",
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #TEST/TRAIN SPLIT\n",
    "# datas = data.iloc[:, 0:2803+7]\n",
    "# classes      = data.iloc[:,-1] #Take PromptReco json\n",
    "# data = datas\n",
    "# # print(classes)\n",
    "\n",
    "# data[2803+7] = runIDs   #Append runID back after scaling\n",
    "# data[2804+7] = lumiIDs  #Append lumiID back after scaling\n",
    "\n",
    "# data['qlabel'] = classes  #qlabel is goldenJSON now\n",
    "\n",
    "# # X_train, X_test = train_test_split(datas, test_size=0.15, random_state=RANDOM_SEED) # This works when we split rndmly\n",
    "# split_nbr = round(data.shape[0]*0.2)  #0.10 means 10% to the test set\n",
    "\n",
    "# # print(datas.shape)\n",
    "# X_train = data.iloc[0:(data.shape[0] - split_nbr) ,:]\n",
    "# X_test  = data.iloc[(data.shape[0] - split_nbr): (data.shape[0]) ,:]\n",
    "# last_train_idx = X_train.shape[0]\n",
    "\n",
    "# np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/last_train_idx.npy', last_train_idx)\n",
    "# # print(X_train.shape)\n",
    "# # print(X_test.shape)\n",
    "\n",
    "# # del(datas)\n",
    "# X_train = X_train[X_train['qlabel']== 0]\n",
    "# # print(X_train)\n",
    "# X_train = X_train.drop(['qlabel'], axis=1)\n",
    "\n",
    "# # ae_lumis = X_train[2800+7].astype(float)\n",
    "# # print(\"ae lumis\", ae_lumis, \"ae_lumis shape\", ae_lumis.shape)\n",
    "# # print(\"XTEEEEST before PerPD json beginn\")\n",
    "# # print(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Normalize the data to make training better\n",
    "\n",
    "# # cutted_data = data.iloc[:, 0:2803+7]\n",
    "# # #classes     = data.iloc[:, 2805:2806] \n",
    "# # classes      = data.iloc[:,-1] #Take PromptReco json\n",
    "\n",
    "\n",
    "# # print(classes.shape)\n",
    "# # X_train = X_train.iloc[:, 0:2803+7]\n",
    "\n",
    "# X_train = StandardScaler().fit_transform(X_train)\n",
    "# # np_scaled = MaxAbsScaler().fit_transform(np_scaled) \n",
    "\n",
    "# # print(\"1111\",np_scaled)\n",
    "\n",
    "\n",
    "# # np_scaled = scale(cutted_data, axis = 1, with_mean=True, with_std=True, copy=True)\n",
    "# X_train = pd.DataFrame(X_train)\n",
    "\n",
    "\n",
    "# classes_X_test      = X_test.iloc[:,-1] #Take PromptReco json\n",
    "\n",
    "# # print(classes.shape)\n",
    "\n",
    "# X_test = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "# # np_scaled = MaxAbsScaler().fit_transform(np_scaled) \n",
    "\n",
    "# # print(\"1111\",np_scaled)\n",
    "\n",
    "\n",
    "# # np_scaled = scale(cutted_data, axis = 1, with_mean=True, with_std=True, copy=True)\n",
    "# X_test = pd.DataFrame(X_test)\n",
    "\n",
    "# X_test['qlabel'] = classes_X_test  #qlabel is goldenJSON now\n",
    "\n",
    "\n",
    "\n",
    "# # datas = pd.DataFrame(np_scaled, index=cutted_data.index, columns=cutted_data.columns)\n",
    "\n",
    "# # print(\"2222\",datas)\n",
    "\n",
    "# # del(np_scaled)\n",
    "# # del(cutted_data)\n",
    "# # print(\"Datas first: \", datas)\n",
    "\n",
    "# # datas['qlabel'] = classes  #qlabel is goldenJSON now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "json_file_path_PD = '/afs/cern.ch/user/f/fsiroky/Documents/gen_config/jsons/JetHT.json'  #Specify what per PD json you want to use for test set\n",
    "\n",
    "def add_flags_from_json_PD(output_json, X_test):\n",
    "    output_json = json.load(open(json_file_path))\n",
    "    new_json_class = np.empty([X_test.shape[0],1])\n",
    "    for i in range(0, X_test.shape[0]):\n",
    "        orig_runid = X_test[2810][i+last_train_idx]\n",
    "        # orig_runid = int(orig_runid)\n",
    "        orig_lumid = X_test[2811][i+last_train_idx]\n",
    "        # orig_lumid = int(orig_lumid)\n",
    "        new_json_class[i,0] = int(json_checker(output_json, orig_runid, orig_lumid))\n",
    "    X_test['PD_json'] = new_json_class\n",
    "    return X_test\n",
    "\n",
    "new_data = add_flags_from_json_PD(json_file_path_PD, X_test)\n",
    "\n",
    "del(new_data)\n",
    "# print(\"Now new X_test label\")\n",
    "# print(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# y_test = X_test['qlabel']\n",
    "\n",
    "y_test = X_test['PD_json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good lumis in X_test:  32076\n",
      "Number of bad lumis in X_test:  661\n"
     ]
    }
   ],
   "source": [
    "#Dropping labels before training and saving Test set luminosities\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of good lumis in X_test: \", len(X_test[y_test==0]))\n",
    "print(\"Number of bad lumis in X_test: \",  len(X_test[y_test==1]))\n",
    "\n",
    "X_test.drop(['qlabel'], axis=1, inplace=True)\n",
    "X_test.drop(['PD_json'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "X_train.drop(2810, axis=1, inplace=True) #drop RunID before training\n",
    "X_train.drop(2811, axis=1, inplace=True) #drop LumiID before training\n",
    "X_test.drop(2810, axis=1, inplace=True) #drop RunID before training\n",
    "X_test.drop(2811, axis=1, inplace=True) #drop LumiID before training\n",
    "\n",
    "\n",
    "# print(\"X_test before saving: \", X_test)\n",
    "\n",
    "luminosity_vals = lumisections.iloc[:int(last_train_idx)].values\n",
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/X_testfor3pds_model.npy', X_test)\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/data_eval/y_testfor3pds_model.npy', y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #TRAINING\n",
    "\n",
    "\n",
    "\n",
    "# from keras.layers import concatenate\n",
    "\n",
    "# from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "# # def custom_activation(x):\n",
    "# #     return ((((x**2+1)**(.5) - 1) / 2 ) + x)\n",
    "\n",
    "# # get_custom_objects().update({'custom_activation': custom_activation})\n",
    "\n",
    "# input_dim = X_train.shape[1]\n",
    "# encoding_dim = 1000\n",
    "\n",
    "\n",
    "# input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "# # prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "\n",
    "# # prellll = LeakyReLU(alpha=0.3)\n",
    "# # encoder = Dense(2600, #activation=\"custom_activation\",\n",
    "# # # kernel_regularizer=regularizers.l2(0.005),\n",
    "# #                 activity_regularizer=regularizers.l1(10e-5) \n",
    "# #                               )(input_layer)\n",
    "# # encoder = prellll(encoder)\n",
    "\n",
    "# # encoder = prellll(encoder)\n",
    "# # luminosity_neuron = Input(shape=(1,))\n",
    "\n",
    "# # luminosity_neuron_dense = Dense(1,)(luminosity_neuron)\n",
    "\n",
    "# # prellll = LeakyReLU(alpha=0.3)\n",
    "# # encoded = Dense(2200, #activation=\"relu\", \n",
    "# # # kernel_regularizer=regularizers.l2(0.005),\n",
    "# #                 # activity_regularizer=regularizers.l1(10e-5)    \n",
    "# #                 )(encoder)\n",
    "# # encoded = prellll(encoded)\n",
    "\n",
    "\n",
    "# # encoded = Dense(2600, activation='relu')(encoder)\n",
    "\n",
    "# # x = concatenate([encoded, luminosity_neuron_dense])\n",
    "# # prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "# encoded = Dense(encoding_dim, #activation=\"relu\", \n",
    "# kernel_regularizer=regularizers.l2(10e-5),\n",
    "#                 # activity_regularizer=regularizers.l1(10e-5) \n",
    "#                    )(input_layer)\n",
    "# encoded = prellll(encoded)\n",
    "\n",
    "# # luminosity_neuron = Input(shape=(1,), name='l_neu')\n",
    "# # decoded = Dense(2600, activation='relu')(encoded)\n",
    "\n",
    "# # x = concatenate([decoded, luminosity_neuron])\n",
    "\n",
    "# # prellll = LeakyReLU(alpha=0.3)\n",
    "# # decoded = Dense(2200, # activation='relu',\n",
    "# #     # activity_regularizer=regularizers.l1(10e-5)\n",
    "# # )(encoded)\n",
    "# # decoded = prellll(decoded)\n",
    "\n",
    "\n",
    "# # prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "\n",
    "# # prellll = LeakyReLU(alpha=0.3)\n",
    "# # decoded = Dense(2600, # activation='relu',\n",
    "# #     # activity_regularizer=regularizers.l1(10e-5)\n",
    "# # )(encoded)\n",
    "# # decoded = prellll(decoded)\n",
    "\n",
    "# # encoder = Dense(int(encoding_dim / 1.2), activation=\"relu\")(encoder)\n",
    "\n",
    "# # encoder = Dense(int(encoding_dim / 1.5), activation=\"relu\")(encoder)\n",
    "\n",
    "# # decoder = Dense(2000, activation='relu')(encoded)\n",
    "# # prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "\n",
    "# decoder = Dense(input_dim)(encoded)\n",
    "# decoder = prellll(decoder)\n",
    "# # decoder = Dense(input_dim)(encoded)\n",
    "\n",
    "# autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_model(input_dim, encoding_dim, activation, activation2, regularizer):\n",
    "#     input_layer = Input(shape=(input_dim, ), name=\"Input\")\n",
    "#     encoded = Dense(encoding_dim, kernel_regularizer=regularizer, name=\"First_Hidden\")(input_layer)\n",
    "#     encoded = activation(encoded)\n",
    "#     decoder = Dense(input_dim, name=\"Output\")(encoded)\n",
    "#     decoder = activation2(decoder)\n",
    "#     return Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_model_foo(input_dim, encoding_dim, activation, activation2, reg_val):\n",
    "#     models = []\n",
    "#     for x in [None, regularizers.l2(reg_val), regularizers.l1(reg_val)]:\n",
    "#         models.append(get_model(X_train.shape[1], encoding_dim, activation, activation2, x))\n",
    "#     return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #TRAINING  #THIS IS USED FOR HYPERPARAMETER SEARCH. ASK ADRIAN.\n",
    "\n",
    "# input_dim = X_train.shape[1]\n",
    "\n",
    "# # MODEL A\n",
    "# activation = LeakyReLU(alpha=0.3, name=\"First_Activation\")\n",
    "# activation2 = LeakyReLU(alpha=0.3, name=\"Second_Activation\")\n",
    "# autoencoderA = get_model_foo(X_train.shape[1], 100, activation, activation2, 10e-5)\n",
    "\n",
    "# # MODEL B\n",
    "# activation = PReLU(alpha_initializer='ones', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"First_Activation\")\n",
    "# activation2 = PReLU(alpha_initializer='ones', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"Second_Activation\")\n",
    "# autoencoderB = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# # MODEL C\n",
    "# activation = LeakyReLU(alpha=0.1, name=\"First_Activation\")\n",
    "# activation2 = LeakyReLU(alpha=0.1, name=\"Second_Activation\")\n",
    "# autoencoderC = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# # MODEL D\n",
    "# activation = LeakyReLU(alpha=0.6, name=\"First_Activation\")\n",
    "# activation2 = LeakyReLU(alpha=0.6, name=\"Second_Activation\")\n",
    "# autoencoderD = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# # MODEL E\n",
    "# from keras.layers import Activation\n",
    "# activation = Activation(\"linear\", name=\"First_Activation\")\n",
    "# activation2 = Activation(\"linear\", name=\"Second_Activation\")\n",
    "# autoencoderE = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)\n",
    "\n",
    "# # MODEL F\n",
    "# activation = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"First_Activation\")\n",
    "# activation2 = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None, name=\"Second_Activation\")\n",
    "# autoencoderF = get_model_foo(X_train.shape[1], 1000, activation, activation2, 10e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'autoencoderA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-71ac5fc59197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mautoencoderA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'autoencoderA' is not defined"
     ]
    }
   ],
   "source": [
    "# for x in autoencoderA:\n",
    "#     x.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=0, batch_size=256):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) not in [2, 3]:\n",
    "                raise ValueError()\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) == 3:\n",
    "                validation_data, validation_targets, validation_set_name = validation_set\n",
    "                sample_weights = None\n",
    "            elif len(validation_set) == 4:\n",
    "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            results = self.model.evaluate(x=validation_data,\n",
    "                                          y=validation_targets,\n",
    "                                          verbose=self.verbose,\n",
    "                                          sample_weight=sample_weights,\n",
    "                                          batch_size=self.batch_size)\n",
    "            \n",
    "            valuename = validation_set_name + '_loss'\n",
    "            print(\"test_loss: \",results)\n",
    "            self.history.setdefault(valuename, []).append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nb_epoch = 8192\n",
    "# batch_size = 256\n",
    "# from keras.optimizers import Adam, Nadam\n",
    "# # adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "\n",
    "# early_stopper = EarlyStopping(monitor=\"val_loss\",\n",
    "#                                   patience=32,\n",
    "#                                   verbose=True,\n",
    "#                                   mode=\"auto\")\n",
    "\n",
    "\n",
    "# for indx1, group in enumerate ([autoencoderA,autoencoderB,autoencoderC,autoencoderD,autoencoderE,autoencoderF]): \n",
    "#     for indx2, autoencoder in enumerate (group):\n",
    "#         name = (\"group%s_autoencoder%s\" % (indx1, indx2))\n",
    "        \n",
    "#         autoencoder.compile(optimizer='Adam', \n",
    "#                             loss='mean_squared_error'\n",
    "#                             # metrics=['accuracy']\n",
    "#                            )\n",
    "\n",
    "#         checkpoint_callback = ModelCheckpoint((\"/afs/cern.ch/user/f/fsiroky/models_ae/%s.h5\" % name),\n",
    "#                                                   monitor=\"val_loss\",\n",
    "#                                                   verbose=False,\n",
    "#                                                   save_best_only=True,\n",
    "#                                                   mode=\"min\")\n",
    "#         testerror = AdditionalValidationSets([(X_test, X_test, 'test')])\n",
    "#         history = autoencoder.fit(X_train, X_train,\n",
    "#                             epochs=nb_epoch,\n",
    "#                             batch_size=batch_size,\n",
    "#                             shuffle=True,\n",
    "#                             validation_split=0.2,\n",
    "#                             verbose=2,\n",
    "#                             callbacks=[testerror, early_stopper, checkpoint_callback]).history\n",
    "\n",
    "#         #np.save('/eos/cms/store/user/fsiroky/ae_models/%s.npy' % name, history)\n",
    "#         np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_loss.npy' % name , history['loss'])\n",
    "#         np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_valloss.npy' % name, history['val_loss'])\n",
    "#         np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_testloss.npy' % name , testerror.history['test_loss'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SINGLE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96425 samples, validate on 32142 samples\n",
      "Epoch 1/8192\n",
      "test_loss:  1.81792132445\n",
      "145s - loss: 9.0094 - val_loss: 1.5253\n",
      "Epoch 2/8192\n",
      "test_loss:  0.954596138681\n",
      "22s - loss: 0.6860 - val_loss: 0.6740\n",
      "Epoch 3/8192\n",
      "test_loss:  0.939976081477\n",
      "20s - loss: 0.5122 - val_loss: 0.6554\n",
      "Epoch 4/8192\n",
      "test_loss:  0.92528392501\n",
      "19s - loss: 0.5031 - val_loss: 0.6470\n",
      "Epoch 5/8192\n",
      "test_loss:  0.918033021473\n",
      "19s - loss: 0.4972 - val_loss: 0.6423\n",
      "Epoch 6/8192\n",
      "test_loss:  0.907677673972\n",
      "23s - loss: 0.4921 - val_loss: 0.6368\n",
      "Epoch 7/8192\n",
      "test_loss:  0.898383662065\n",
      "19s - loss: 0.4871 - val_loss: 0.6303\n",
      "Epoch 8/8192\n",
      "test_loss:  0.88881926086\n",
      "19s - loss: 0.4828 - val_loss: 0.6248\n",
      "Epoch 9/8192\n",
      "test_loss:  0.879108742396\n",
      "20s - loss: 0.4792 - val_loss: 0.6185\n",
      "Epoch 10/8192\n",
      "test_loss:  0.872491444603\n",
      "19s - loss: 0.4762 - val_loss: 0.6146\n",
      "Epoch 11/8192\n",
      "test_loss:  0.871568756913\n",
      "19s - loss: 0.4739 - val_loss: 0.6120\n",
      "Epoch 12/8192\n",
      "test_loss:  0.867483794367\n",
      "20s - loss: 0.4721 - val_loss: 0.6097\n",
      "Epoch 13/8192\n",
      "test_loss:  0.865887304767\n",
      "19s - loss: 0.4705 - val_loss: 0.6085\n",
      "Epoch 14/8192\n",
      "test_loss:  0.862664345182\n",
      "20s - loss: 0.4691 - val_loss: 0.6073\n",
      "Epoch 15/8192\n",
      "test_loss:  0.862851894768\n",
      "20s - loss: 0.4679 - val_loss: 0.6053\n",
      "Epoch 16/8192\n",
      "test_loss:  0.860269638061\n",
      "20s - loss: 0.4668 - val_loss: 0.6021\n",
      "Epoch 17/8192\n",
      "test_loss:  0.857843972372\n",
      "20s - loss: 0.4657 - val_loss: 0.5991\n",
      "Epoch 18/8192\n",
      "test_loss:  0.857265079206\n",
      "19s - loss: 0.4646 - val_loss: 0.5966\n",
      "Epoch 19/8192\n",
      "test_loss:  0.858328840408\n",
      "19s - loss: 0.4635 - val_loss: 0.5956\n",
      "Epoch 20/8192\n",
      "test_loss:  0.852485473334\n",
      "20s - loss: 0.4624 - val_loss: 0.5908\n",
      "Epoch 21/8192\n",
      "test_loss:  0.855058097776\n",
      "19s - loss: 0.4613 - val_loss: 0.5881\n",
      "Epoch 22/8192\n",
      "test_loss:  0.847639200131\n",
      "19s - loss: 0.4601 - val_loss: 0.5805\n",
      "Epoch 23/8192\n",
      "test_loss:  0.853683018064\n",
      "19s - loss: 0.4589 - val_loss: 0.5775\n",
      "Epoch 24/8192\n",
      "test_loss:  0.848556977542\n",
      "20s - loss: 0.4575 - val_loss: 0.5680\n",
      "Epoch 25/8192\n",
      "test_loss:  0.848632841697\n",
      "19s - loss: 0.4561 - val_loss: 0.5614\n",
      "Epoch 26/8192\n",
      "test_loss:  0.839199354516\n",
      "20s - loss: 0.4547 - val_loss: 0.5529\n",
      "Epoch 27/8192\n",
      "test_loss:  0.84378908622\n",
      "21s - loss: 0.4536 - val_loss: 0.5491\n",
      "Epoch 28/8192\n",
      "test_loss:  0.83916718987\n",
      "19s - loss: 0.4526 - val_loss: 0.5440\n",
      "Epoch 29/8192\n",
      "test_loss:  0.83643956118\n",
      "20s - loss: 0.4517 - val_loss: 0.5412\n",
      "Epoch 30/8192\n",
      "test_loss:  0.831576617789\n",
      "19s - loss: 0.4509 - val_loss: 0.5385\n",
      "Epoch 31/8192\n",
      "test_loss:  0.828419123647\n",
      "19s - loss: 0.4499 - val_loss: 0.5334\n",
      "Epoch 32/8192\n",
      "test_loss:  0.829604605676\n",
      "19s - loss: 0.4485 - val_loss: 0.5294\n",
      "Epoch 33/8192\n",
      "test_loss:  0.82309323448\n",
      "19s - loss: 0.4469 - val_loss: 0.5245\n",
      "Epoch 34/8192\n",
      "test_loss:  0.819705526499\n",
      "20s - loss: 0.4452 - val_loss: 0.5201\n",
      "Epoch 35/8192\n",
      "test_loss:  0.820138658188\n",
      "20s - loss: 0.4438 - val_loss: 0.5168\n",
      "Epoch 36/8192\n",
      "test_loss:  0.816866115664\n",
      "20s - loss: 0.4426 - val_loss: 0.5137\n",
      "Epoch 37/8192\n",
      "test_loss:  0.814939001002\n",
      "20s - loss: 0.4417 - val_loss: 0.5120\n",
      "Epoch 38/8192\n",
      "test_loss:  0.813423215465\n",
      "21s - loss: 0.4408 - val_loss: 0.5106\n",
      "Epoch 39/8192\n",
      "test_loss:  0.813092564018\n",
      "19s - loss: 0.4402 - val_loss: 0.5092\n",
      "Epoch 40/8192\n",
      "test_loss:  0.808664990044\n",
      "20s - loss: 0.4396 - val_loss: 0.5065\n",
      "Epoch 41/8192\n",
      "test_loss:  0.808165614066\n",
      "21s - loss: 0.4390 - val_loss: 0.5064\n",
      "Epoch 42/8192\n",
      "test_loss:  0.80611390404\n",
      "20s - loss: 0.4384 - val_loss: 0.5061\n",
      "Epoch 43/8192\n",
      "test_loss:  0.806978129778\n",
      "20s - loss: 0.4380 - val_loss: 0.5050\n",
      "Epoch 44/8192\n",
      "test_loss:  0.803541562517\n",
      "21s - loss: 0.4375 - val_loss: 0.5045\n",
      "Epoch 45/8192\n",
      "test_loss:  0.805345677001\n",
      "20s - loss: 0.4371 - val_loss: 0.5044\n",
      "Epoch 46/8192\n",
      "test_loss:  0.803312906425\n",
      "21s - loss: 0.4367 - val_loss: 0.5033\n",
      "Epoch 47/8192\n",
      "test_loss:  0.803994086238\n",
      "18s - loss: 0.4362 - val_loss: 0.5034\n",
      "Epoch 48/8192\n",
      "test_loss:  0.802590012053\n",
      "19s - loss: 0.4358 - val_loss: 0.5024\n",
      "Epoch 49/8192\n",
      "test_loss:  0.801718625216\n",
      "18s - loss: 0.4355 - val_loss: 0.5024\n",
      "Epoch 50/8192\n",
      "test_loss:  0.799608603374\n",
      "21s - loss: 0.4350 - val_loss: 0.5012\n",
      "Epoch 51/8192\n",
      "test_loss:  0.801042925479\n",
      "17s - loss: 0.4347 - val_loss: 0.5016\n",
      "Epoch 52/8192\n",
      "test_loss:  0.800542463994\n",
      "17s - loss: 0.4343 - val_loss: 0.5022\n",
      "Epoch 53/8192\n",
      "test_loss:  0.800060896155\n",
      "16s - loss: 0.4340 - val_loss: 0.5017\n",
      "Epoch 54/8192\n",
      "test_loss:  0.795838805454\n",
      "19s - loss: 0.4337 - val_loss: 0.5004\n",
      "Epoch 55/8192\n",
      "test_loss:  0.798956179033\n",
      "17s - loss: 0.4334 - val_loss: 0.5007\n",
      "Epoch 56/8192\n",
      "test_loss:  0.796246694687\n",
      "19s - loss: 0.4331 - val_loss: 0.5000\n",
      "Epoch 57/8192\n",
      "test_loss:  0.796134500156\n",
      "16s - loss: 0.4329 - val_loss: 0.5000\n",
      "Epoch 58/8192\n",
      "test_loss:  0.799025574623\n",
      "15s - loss: 0.4327 - val_loss: 0.5014\n",
      "Epoch 59/8192\n",
      "test_loss:  0.795420434321\n",
      "15s - loss: 0.4324 - val_loss: 0.5004\n",
      "Epoch 60/8192\n",
      "test_loss:  0.796102203242\n",
      "16s - loss: 0.4322 - val_loss: 0.5001\n",
      "Epoch 61/8192\n",
      "test_loss:  0.797485583807\n",
      "19s - loss: 0.4319 - val_loss: 0.4999\n",
      "Epoch 62/8192\n",
      "test_loss:  0.796392243398\n",
      "20s - loss: 0.4317 - val_loss: 0.4995\n",
      "Epoch 63/8192\n",
      "test_loss:  0.798070466103\n",
      "17s - loss: 0.4315 - val_loss: 0.5001\n",
      "Epoch 64/8192\n",
      "test_loss:  0.795397412924\n",
      "19s - loss: 0.4313 - val_loss: 0.4991\n",
      "Epoch 65/8192\n",
      "test_loss:  0.795715882431\n",
      "16s - loss: 0.4311 - val_loss: 0.4994\n",
      "Epoch 66/8192\n",
      "test_loss:  0.794550909068\n",
      "19s - loss: 0.4309 - val_loss: 0.4989\n",
      "Epoch 67/8192\n",
      "test_loss:  0.794992275363\n",
      "18s - loss: 0.4307 - val_loss: 0.4998\n",
      "Epoch 68/8192\n",
      "test_loss:  0.794134692801\n",
      "20s - loss: 0.4305 - val_loss: 0.4986\n",
      "Epoch 69/8192\n",
      "test_loss:  0.797460903185\n",
      "17s - loss: 0.4304 - val_loss: 0.4998\n",
      "Epoch 70/8192\n",
      "test_loss:  0.796217599116\n",
      "17s - loss: 0.4302 - val_loss: 0.4988\n",
      "Epoch 71/8192\n",
      "test_loss:  0.795717649317\n",
      "16s - loss: 0.4301 - val_loss: 0.4990\n",
      "Epoch 72/8192\n",
      "test_loss:  0.800088334082\n",
      "16s - loss: 0.4299 - val_loss: 0.5003\n",
      "Epoch 73/8192\n",
      "test_loss:  0.796880708842\n",
      "19s - loss: 0.4297 - val_loss: 0.4983\n",
      "Epoch 74/8192\n",
      "test_loss:  0.797922354707\n",
      "15s - loss: 0.4296 - val_loss: 0.4992\n",
      "Epoch 75/8192\n",
      "test_loss:  0.800928032706\n",
      "15s - loss: 0.4294 - val_loss: 0.4998\n",
      "Epoch 76/8192\n",
      "test_loss:  0.794103312916\n",
      "18s - loss: 0.4293 - val_loss: 0.4980\n",
      "Epoch 77/8192\n",
      "test_loss:  0.794335224769\n",
      "19s - loss: 0.4292 - val_loss: 0.4978\n",
      "Epoch 78/8192\n",
      "test_loss:  0.797550813718\n",
      "16s - loss: 0.4290 - val_loss: 0.4984\n",
      "Epoch 79/8192\n",
      "test_loss:  0.797791784392\n",
      "16s - loss: 0.4289 - val_loss: 0.4992\n",
      "Epoch 80/8192\n",
      "test_loss:  0.79661161415\n",
      "16s - loss: 0.4287 - val_loss: 0.4982\n",
      "Epoch 81/8192\n",
      "test_loss:  0.797281307607\n",
      "15s - loss: 0.4286 - val_loss: 0.4990\n",
      "Epoch 82/8192\n",
      "test_loss:  0.797233799124\n",
      "16s - loss: 0.4285 - val_loss: 0.4980\n",
      "Epoch 83/8192\n",
      "test_loss:  0.800832934057\n",
      "16s - loss: 0.4284 - val_loss: 0.4997\n",
      "Epoch 84/8192\n",
      "test_loss:  0.798624402098\n",
      "15s - loss: 0.4282 - val_loss: 0.4994\n",
      "Epoch 85/8192\n",
      "test_loss:  0.799666942357\n",
      "15s - loss: 0.4280 - val_loss: 0.4986\n",
      "Epoch 86/8192\n",
      "test_loss:  0.800563292322\n",
      "15s - loss: 0.4278 - val_loss: 0.4997\n",
      "Epoch 87/8192\n",
      "test_loss:  0.80095190033\n",
      "15s - loss: 0.4277 - val_loss: 0.5002\n",
      "Epoch 88/8192\n",
      "test_loss:  0.80135187261\n",
      "15s - loss: 0.4275 - val_loss: 0.4998\n",
      "Epoch 89/8192\n",
      "test_loss:  0.804089080429\n",
      "15s - loss: 0.4274 - val_loss: 0.4997\n",
      "Epoch 90/8192\n",
      "test_loss:  0.807023146188\n",
      "16s - loss: 0.4271 - val_loss: 0.5013\n",
      "Epoch 91/8192\n",
      "test_loss:  0.799898356697\n",
      "16s - loss: 0.4270 - val_loss: 0.5010\n",
      "Epoch 92/8192\n",
      "test_loss:  0.808940284005\n",
      "15s - loss: 0.4267 - val_loss: 0.5015\n",
      "Epoch 93/8192\n",
      "test_loss:  0.804301627976\n",
      "15s - loss: 0.4264 - val_loss: 0.5005\n",
      "Epoch 94/8192\n",
      "test_loss:  0.808528180679\n",
      "15s - loss: 0.4262 - val_loss: 0.4986\n",
      "Epoch 95/8192\n",
      "test_loss:  0.80296462641\n",
      "16s - loss: 0.4260 - val_loss: 0.4994\n",
      "Epoch 96/8192\n",
      "test_loss:  0.809910698571\n",
      "16s - loss: 0.4257 - val_loss: 0.5011\n",
      "Epoch 97/8192\n",
      "test_loss:  0.807027823229\n",
      "15s - loss: 0.4255 - val_loss: 0.5000\n",
      "Epoch 98/8192\n",
      "test_loss:  0.803125375554\n",
      "16s - loss: 0.4254 - val_loss: 0.5003\n",
      "Epoch 99/8192\n",
      "test_loss:  0.810956172709\n",
      "16s - loss: 0.4251 - val_loss: 0.4984\n",
      "Epoch 100/8192\n",
      "test_loss:  0.811365812814\n",
      "16s - loss: 0.4250 - val_loss: 0.5010\n",
      "Epoch 101/8192\n",
      "test_loss:  0.808438254165\n",
      "17s - loss: 0.4249 - val_loss: 0.4993\n",
      "Epoch 102/8192\n",
      "test_loss:  0.807762147525\n",
      "16s - loss: 0.4249 - val_loss: 0.4993\n",
      "Epoch 103/8192\n",
      "test_loss:  0.810864205978\n",
      "15s - loss: 0.4247 - val_loss: 0.4988\n",
      "Epoch 104/8192\n",
      "test_loss:  0.810980045949\n",
      "15s - loss: 0.4247 - val_loss: 0.5010\n",
      "Epoch 105/8192\n",
      "test_loss:  0.809355667768\n",
      "16s - loss: 0.4245 - val_loss: 0.4995\n",
      "Epoch 106/8192\n",
      "test_loss:  0.801261834591\n",
      "16s - loss: 0.4245 - val_loss: 0.4992\n",
      "Epoch 107/8192\n",
      "test_loss:  0.805595776078\n",
      "17s - loss: 0.4244 - val_loss: 0.4999\n",
      "Epoch 108/8192\n",
      "test_loss:  0.809596425297\n",
      "17s - loss: 0.4243 - val_loss: 0.4990\n",
      "Epoch 109/8192\n",
      "test_loss:  0.813646401754\n",
      "16s - loss: 0.4242 - val_loss: 0.4987\n",
      "Epoch 110/8192\n",
      "test_loss:  0.806906670191\n",
      "19s - loss: 0.4241 - val_loss: 0.4977\n",
      "Epoch 111/8192\n",
      "test_loss:  0.806696116695\n",
      "15s - loss: 0.4240 - val_loss: 0.4981\n",
      "Epoch 112/8192\n",
      "test_loss:  0.816228270968\n",
      "15s - loss: 0.4240 - val_loss: 0.5001\n",
      "Epoch 113/8192\n",
      "test_loss:  0.805219133323\n",
      "15s - loss: 0.4240 - val_loss: 0.4993\n",
      "Epoch 114/8192\n",
      "test_loss:  0.811487464259\n",
      "15s - loss: 0.4240 - val_loss: 0.5002\n",
      "Epoch 115/8192\n",
      "test_loss:  0.804700181635\n",
      "15s - loss: 0.4239 - val_loss: 0.4993\n",
      "Epoch 116/8192\n",
      "test_loss:  0.804216748866\n",
      "15s - loss: 0.4237 - val_loss: 0.4989\n",
      "Epoch 117/8192\n",
      "test_loss:  0.801335473598\n",
      "15s - loss: 0.4237 - val_loss: 0.4986\n",
      "Epoch 118/8192\n",
      "test_loss:  0.804451975206\n",
      "15s - loss: 0.4237 - val_loss: 0.4986\n",
      "Epoch 119/8192\n",
      "test_loss:  0.808803390955\n",
      "17s - loss: 0.4236 - val_loss: 0.5009\n",
      "Epoch 120/8192\n",
      "test_loss:  0.804455376831\n",
      "16s - loss: 0.4236 - val_loss: 0.4984\n",
      "Epoch 121/8192\n",
      "test_loss:  0.810476565878\n",
      "15s - loss: 0.4235 - val_loss: 0.4995\n",
      "Epoch 122/8192\n",
      "test_loss:  0.803320126835\n",
      "15s - loss: 0.4234 - val_loss: 0.4978\n",
      "Epoch 123/8192\n",
      "test_loss:  0.810683378216\n",
      "15s - loss: 0.4233 - val_loss: 0.5008\n",
      "Epoch 124/8192\n",
      "test_loss:  0.812240187734\n",
      "15s - loss: 0.4233 - val_loss: 0.4987\n",
      "Epoch 125/8192\n",
      "test_loss:  0.806846857355\n",
      "15s - loss: 0.4233 - val_loss: 0.4983\n",
      "Epoch 126/8192\n",
      "test_loss:  0.809881541049\n",
      "16s - loss: 0.4232 - val_loss: 0.5008\n",
      "Epoch 127/8192\n",
      "test_loss:  0.805039988813\n",
      "16s - loss: 0.4232 - val_loss: 0.4998\n",
      "Epoch 128/8192\n",
      "test_loss:  0.804114989829\n",
      "20s - loss: 0.4231 - val_loss: 0.4962\n",
      "Epoch 129/8192\n",
      "test_loss:  0.802900703131\n",
      "16s - loss: 0.4232 - val_loss: 0.4976\n",
      "Epoch 130/8192\n",
      "test_loss:  0.80881478219\n",
      "15s - loss: 0.4231 - val_loss: 0.4976\n",
      "Epoch 131/8192\n",
      "test_loss:  0.808312687734\n",
      "15s - loss: 0.4230 - val_loss: 0.4984\n",
      "Epoch 132/8192\n",
      "test_loss:  0.804440952722\n",
      "15s - loss: 0.4230 - val_loss: 0.4976\n",
      "Epoch 133/8192\n",
      "test_loss:  0.809491602133\n",
      "15s - loss: 0.4229 - val_loss: 0.4990\n",
      "Epoch 134/8192\n",
      "test_loss:  0.804458054304\n",
      "16s - loss: 0.4228 - val_loss: 0.4977\n",
      "Epoch 135/8192\n",
      "test_loss:  0.807307306752\n",
      "15s - loss: 0.4228 - val_loss: 0.4980\n",
      "Epoch 136/8192\n",
      "test_loss:  0.801372933292\n",
      "16s - loss: 0.4228 - val_loss: 0.4963\n",
      "Epoch 137/8192\n",
      "test_loss:  0.807522329123\n",
      "16s - loss: 0.4227 - val_loss: 0.4972\n",
      "Epoch 138/8192\n",
      "test_loss:  0.803565370415\n",
      "16s - loss: 0.4227 - val_loss: 0.4980\n",
      "Epoch 139/8192\n",
      "test_loss:  0.802072123794\n",
      "16s - loss: 0.4226 - val_loss: 0.4971\n",
      "Epoch 140/8192\n",
      "test_loss:  0.798648008139\n",
      "15s - loss: 0.4226 - val_loss: 0.4972\n",
      "Epoch 141/8192\n",
      "test_loss:  0.802565614702\n",
      "15s - loss: 0.4225 - val_loss: 0.4968\n",
      "Epoch 142/8192\n",
      "test_loss:  0.80705856911\n",
      "16s - loss: 0.4225 - val_loss: 0.4992\n",
      "Epoch 143/8192\n",
      "test_loss:  0.811297689927\n",
      "15s - loss: 0.4225 - val_loss: 0.4986\n",
      "Epoch 144/8192\n",
      "test_loss:  0.808705770098\n",
      "17s - loss: 0.4224 - val_loss: 0.4972\n",
      "Epoch 145/8192\n",
      "test_loss:  0.802422936959\n",
      "16s - loss: 0.4221 - val_loss: 0.4987\n",
      "Epoch 146/8192\n",
      "test_loss:  0.806827075619\n",
      "15s - loss: 0.4221 - val_loss: 0.4968\n",
      "Epoch 147/8192\n",
      "test_loss:  0.807489445923\n",
      "16s - loss: 0.4220 - val_loss: 0.4992\n",
      "Epoch 148/8192\n",
      "test_loss:  0.808551106206\n",
      "16s - loss: 0.4220 - val_loss: 0.4994\n",
      "Epoch 149/8192\n",
      "test_loss:  0.80601200507\n",
      "16s - loss: 0.4218 - val_loss: 0.4975\n",
      "Epoch 150/8192\n",
      "test_loss:  0.804908271601\n",
      "16s - loss: 0.4218 - val_loss: 0.4984\n",
      "Epoch 151/8192\n",
      "test_loss:  0.810942312928\n",
      "16s - loss: 0.4218 - val_loss: 0.4982\n",
      "Epoch 152/8192\n",
      "test_loss:  0.808246916029\n",
      "16s - loss: 0.4218 - val_loss: 0.4966\n",
      "Epoch 153/8192\n",
      "test_loss:  0.810365762535\n",
      "16s - loss: 0.4219 - val_loss: 0.4975\n",
      "Epoch 154/8192\n",
      "test_loss:  0.799367297121\n",
      "20s - loss: 0.4217 - val_loss: 0.4953\n",
      "Epoch 155/8192\n",
      "test_loss:  0.809647404045\n",
      "16s - loss: 0.4217 - val_loss: 0.4984\n",
      "Epoch 156/8192\n",
      "test_loss:  0.811159361333\n",
      "16s - loss: 0.4216 - val_loss: 0.4976\n",
      "Epoch 157/8192\n",
      "test_loss:  0.810008066472\n",
      "16s - loss: 0.4215 - val_loss: 0.4997\n",
      "Epoch 158/8192\n",
      "test_loss:  0.811687747997\n",
      "17s - loss: 0.4215 - val_loss: 0.4986\n",
      "Epoch 159/8192\n",
      "test_loss:  0.804376841877\n",
      "16s - loss: 0.4215 - val_loss: 0.4972\n",
      "Epoch 160/8192\n",
      "test_loss:  0.810389826205\n",
      "16s - loss: 0.4213 - val_loss: 0.4973\n",
      "Epoch 161/8192\n",
      "test_loss:  0.813632127491\n",
      "17s - loss: 0.4214 - val_loss: 0.4986\n",
      "Epoch 162/8192\n",
      "test_loss:  0.810196983154\n",
      "17s - loss: 0.4214 - val_loss: 0.4973\n",
      "Epoch 163/8192\n",
      "test_loss:  0.805626208593\n",
      "16s - loss: 0.4214 - val_loss: 0.4963\n",
      "Epoch 164/8192\n",
      "test_loss:  0.805115258919\n",
      "16s - loss: 0.4213 - val_loss: 0.4969\n",
      "Epoch 165/8192\n",
      "test_loss:  0.803174959015\n",
      "16s - loss: 0.4213 - val_loss: 0.4969\n",
      "Epoch 166/8192\n",
      "test_loss:  0.804626725528\n",
      "16s - loss: 0.4212 - val_loss: 0.4982\n",
      "Epoch 167/8192\n",
      "test_loss:  0.810830428952\n",
      "17s - loss: 0.4211 - val_loss: 0.4982\n",
      "Epoch 168/8192\n",
      "test_loss:  0.810812219836\n",
      "17s - loss: 0.4212 - val_loss: 0.4974\n",
      "Epoch 169/8192\n",
      "test_loss:  0.805901285702\n",
      "17s - loss: 0.4211 - val_loss: 0.4967\n",
      "Epoch 170/8192\n",
      "test_loss:  0.798708268291\n",
      "19s - loss: 0.4210 - val_loss: 0.4949\n",
      "Epoch 171/8192\n",
      "test_loss:  0.809431862335\n",
      "16s - loss: 0.4210 - val_loss: 0.4986\n",
      "Epoch 172/8192\n",
      "test_loss:  0.803653316388\n",
      "16s - loss: 0.4209 - val_loss: 0.4960\n",
      "Epoch 173/8192\n",
      "test_loss:  0.809177783069\n",
      "16s - loss: 0.4208 - val_loss: 0.4977\n",
      "Epoch 174/8192\n",
      "test_loss:  0.809683867933\n",
      "17s - loss: 0.4209 - val_loss: 0.4984\n",
      "Epoch 175/8192\n",
      "test_loss:  0.806868570497\n",
      "16s - loss: 0.4209 - val_loss: 0.4961\n",
      "Epoch 176/8192\n",
      "test_loss:  0.807864343444\n",
      "16s - loss: 0.4209 - val_loss: 0.4961\n",
      "Epoch 177/8192\n",
      "test_loss:  0.817063690009\n",
      "16s - loss: 0.4207 - val_loss: 0.4981\n",
      "Epoch 178/8192\n",
      "test_loss:  0.803673578726\n",
      "17s - loss: 0.4207 - val_loss: 0.4965\n",
      "Epoch 179/8192\n",
      "test_loss:  0.809914074213\n",
      "16s - loss: 0.4209 - val_loss: 0.4965\n",
      "Epoch 180/8192\n",
      "test_loss:  0.798586409485\n",
      "16s - loss: 0.4207 - val_loss: 0.4973\n",
      "Epoch 181/8192\n",
      "test_loss:  0.809442619372\n",
      "16s - loss: 0.4207 - val_loss: 0.4988\n",
      "Epoch 182/8192\n",
      "test_loss:  0.812213302314\n",
      "16s - loss: 0.4207 - val_loss: 0.4988\n",
      "Epoch 183/8192\n",
      "test_loss:  0.813952284248\n",
      "16s - loss: 0.4205 - val_loss: 0.4975\n",
      "Epoch 184/8192\n",
      "test_loss:  0.801306767359\n",
      "16s - loss: 0.4204 - val_loss: 0.4956\n",
      "Epoch 185/8192\n",
      "test_loss:  0.808991635496\n",
      "16s - loss: 0.4204 - val_loss: 0.4967\n",
      "Epoch 186/8192\n",
      "test_loss:  0.80943019687\n",
      "15s - loss: 0.4206 - val_loss: 0.4981\n",
      "Epoch 187/8192\n",
      "test_loss:  0.806460856993\n",
      "16s - loss: 0.4205 - val_loss: 0.4980\n",
      "Epoch 188/8192\n",
      "test_loss:  0.80325776286\n",
      "17s - loss: 0.4204 - val_loss: 0.4953\n",
      "Epoch 189/8192\n",
      "test_loss:  0.807423504804\n",
      "17s - loss: 0.4203 - val_loss: 0.4957\n",
      "Epoch 190/8192\n",
      "test_loss:  0.810908930811\n",
      "16s - loss: 0.4203 - val_loss: 0.4981\n",
      "Epoch 191/8192\n",
      "test_loss:  0.801522268002\n",
      "23s - loss: 0.4204 - val_loss: 0.4945\n",
      "Epoch 192/8192\n",
      "test_loss:  0.809706367389\n",
      "17s - loss: 0.4203 - val_loss: 0.4967\n",
      "Epoch 193/8192\n",
      "test_loss:  0.809735738858\n",
      "16s - loss: 0.4201 - val_loss: 0.4977\n",
      "Epoch 194/8192\n",
      "test_loss:  0.804635648458\n",
      "16s - loss: 0.4201 - val_loss: 0.4967\n",
      "Epoch 195/8192\n",
      "test_loss:  0.80331673185\n",
      "16s - loss: 0.4202 - val_loss: 0.4961\n",
      "Epoch 196/8192\n",
      "test_loss:  0.807500885343\n",
      "16s - loss: 0.4202 - val_loss: 0.4981\n",
      "Epoch 197/8192\n",
      "test_loss:  0.804787829678\n",
      "16s - loss: 0.4200 - val_loss: 0.4991\n",
      "Epoch 198/8192\n",
      "test_loss:  0.810094961448\n",
      "16s - loss: 0.4200 - val_loss: 0.4961\n",
      "Epoch 199/8192\n",
      "test_loss:  0.810297579017\n",
      "15s - loss: 0.4200 - val_loss: 0.4975\n",
      "Epoch 200/8192\n",
      "test_loss:  0.814402687649\n",
      "18s - loss: 0.4200 - val_loss: 0.4981\n",
      "Epoch 201/8192\n",
      "test_loss:  0.810339878035\n",
      "18s - loss: 0.4198 - val_loss: 0.4967\n",
      "Epoch 202/8192\n",
      "test_loss:  0.811158188743\n",
      "16s - loss: 0.4198 - val_loss: 0.4976\n",
      "Epoch 203/8192\n",
      "test_loss:  0.809459095112\n",
      "16s - loss: 0.4197 - val_loss: 0.4973\n",
      "Epoch 204/8192\n",
      "test_loss:  0.805955606903\n",
      "18s - loss: 0.4199 - val_loss: 0.4946\n",
      "Epoch 205/8192\n",
      "test_loss:  0.808928575841\n",
      "17s - loss: 0.4198 - val_loss: 0.4968\n",
      "Epoch 206/8192\n",
      "test_loss:  0.811485900333\n",
      "17s - loss: 0.4197 - val_loss: 0.4962\n",
      "Epoch 207/8192\n",
      "test_loss:  0.810519938396\n",
      "17s - loss: 0.4196 - val_loss: 0.4965\n",
      "Epoch 208/8192\n",
      "test_loss:  0.808217172967\n",
      "16s - loss: 0.4197 - val_loss: 0.4985\n",
      "Epoch 209/8192\n",
      "test_loss:  0.806744497323\n",
      "16s - loss: 0.4196 - val_loss: 0.4965\n",
      "Epoch 210/8192\n",
      "test_loss:  0.810909310227\n",
      "17s - loss: 0.4195 - val_loss: 0.4967\n",
      "Epoch 211/8192\n",
      "test_loss:  0.80230857394\n",
      "16s - loss: 0.4196 - val_loss: 0.4970\n",
      "Epoch 212/8192\n",
      "test_loss:  0.810883912556\n",
      "16s - loss: 0.4194 - val_loss: 0.4979\n",
      "Epoch 213/8192\n",
      "test_loss:  0.804124118651\n",
      "16s - loss: 0.4194 - val_loss: 0.4975\n",
      "Epoch 214/8192\n",
      "test_loss:  0.814984649006\n",
      "16s - loss: 0.4193 - val_loss: 0.4968\n",
      "Epoch 215/8192\n",
      "test_loss:  0.806589854402\n",
      "17s - loss: 0.4191 - val_loss: 0.4992\n",
      "Epoch 216/8192\n",
      "test_loss:  0.810189792009\n",
      "17s - loss: 0.4192 - val_loss: 0.4950\n",
      "Epoch 217/8192\n",
      "test_loss:  0.823735120203\n",
      "16s - loss: 0.4191 - val_loss: 0.5032\n",
      "Epoch 218/8192\n",
      "test_loss:  0.809313792962\n",
      "16s - loss: 0.4190 - val_loss: 0.4968\n",
      "Epoch 219/8192\n",
      "test_loss:  0.808101003077\n",
      "17s - loss: 0.4188 - val_loss: 0.4965\n",
      "Epoch 220/8192\n",
      "test_loss:  0.806350162253\n",
      "17s - loss: 0.4186 - val_loss: 0.4982\n",
      "Epoch 221/8192\n",
      "test_loss:  0.804172440264\n",
      "16s - loss: 0.4186 - val_loss: 0.4974\n",
      "Epoch 222/8192\n",
      "test_loss:  0.81328954852\n",
      "16s - loss: 0.4185 - val_loss: 0.4979\n",
      "Epoch 223/8192\n",
      "test_loss:  0.81674445139\n",
      "16s - loss: 0.4183 - val_loss: 0.4982\n",
      "Epoch 224/8192\n",
      "test_loss:  0.813207992757\n",
      "17s - loss: 0.4181 - val_loss: 0.4981\n",
      "Epoch 225/8192\n",
      "test_loss:  0.804893245602\n",
      "18s - loss: 0.4182 - val_loss: 0.4950\n",
      "Epoch 226/8192\n",
      "test_loss:  0.811043172165\n",
      "16s - loss: 0.4182 - val_loss: 0.4986\n",
      "Epoch 227/8192\n",
      "test_loss:  0.812253009214\n",
      "16s - loss: 0.4181 - val_loss: 0.4973\n",
      "Epoch 228/8192\n",
      "test_loss:  0.805285828918\n",
      "16s - loss: 0.4180 - val_loss: 0.4997\n",
      "Epoch 229/8192\n",
      "test_loss:  0.820449431549\n",
      "17s - loss: 0.4178 - val_loss: 0.5008\n",
      "Epoch 230/8192\n",
      "test_loss:  0.809333785535\n",
      "16s - loss: 0.4183 - val_loss: 0.4977\n",
      "Epoch 231/8192\n",
      "test_loss:  0.809278617561\n",
      "16s - loss: 0.4181 - val_loss: 0.4973\n",
      "Epoch 232/8192\n",
      "test_loss:  0.812323117928\n",
      "16s - loss: 0.4176 - val_loss: 0.4998\n",
      "Epoch 233/8192\n",
      "test_loss:  0.814070250986\n",
      "17s - loss: 0.4181 - val_loss: 0.5004\n",
      "Epoch 234/8192\n",
      "test_loss:  0.813742658449\n",
      "16s - loss: 0.4175 - val_loss: 0.4983\n",
      "Epoch 235/8192\n",
      "test_loss:  0.811734628624\n",
      "16s - loss: 0.4173 - val_loss: 0.4963\n",
      "Epoch 236/8192\n",
      "test_loss:  0.816110576128\n",
      "17s - loss: 0.4171 - val_loss: 0.4991\n",
      "Epoch 237/8192\n",
      "test_loss:  0.812884521952\n",
      "17s - loss: 0.4170 - val_loss: 0.5015\n",
      "Epoch 238/8192\n",
      "test_loss:  0.807024213471\n",
      "16s - loss: 0.4171 - val_loss: 0.4965\n",
      "Epoch 239/8192\n",
      "test_loss:  0.817254556122\n",
      "16s - loss: 0.4171 - val_loss: 0.4998\n",
      "Epoch 240/8192\n",
      "test_loss:  0.815464191099\n",
      "17s - loss: 0.4170 - val_loss: 0.4976\n",
      "Epoch 241/8192\n",
      "test_loss:  0.815276674262\n",
      "17s - loss: 0.4170 - val_loss: 0.4981\n",
      "Epoch 242/8192\n",
      "test_loss:  0.806687682906\n",
      "16s - loss: 0.4171 - val_loss: 0.4965\n",
      "Epoch 00241: early stopping\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 8192\n",
    "batch_size = 256\n",
    "from keras import optimizers\n",
    "\n",
    "#Adam and similar optimizers dont work with epsilon=0 for this version of KERAS.\n",
    "#Make sure you check the version of keras and find appropriate documentation for that version \n",
    "adamm = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1, clipnorm=1.)\n",
    "\n",
    "ada = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 500\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(2000, #activation=\"relu\", \n",
    "kernel_regularizer=regularizers.l1(10e-5),\n",
    "#                 activity_regularizer=regularizers.l2(10e-5) \n",
    "                   )(input_layer)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(1000, #activation=\"relu\", \n",
    "kernel_regularizer=regularizers.l1(10e-5),\n",
    "#                 activity_regularizer=regularizers.l2(10e-5) \n",
    "                   )(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "encoded = Dense(encoding_dim, #activation=\"relu\", \n",
    "kernel_regularizer=regularizers.l1(10e-5),\n",
    "#                 activity_regularizer=regularizers.l2(10e-5) \n",
    "                   )(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(1000, #activation=\"relu\", \n",
    "kernel_regularizer=regularizers.l1(10e-5),\n",
    "#                 activity_regularizer=regularizers.l1(10e-5) \n",
    "                   )(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "encoded = Dense(2000, #activation=\"relu\", \n",
    "kernel_regularizer=regularizers.l1(10e-5),\n",
    "#                 activity_regularizer=regularizers.l2(10e-5) \n",
    "                   )(encoded)\n",
    "encoded = prellll(encoded)\n",
    "\n",
    "prellll = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)\n",
    "# prellll = LeakyReLU(alpha=0.3)\n",
    "\n",
    "decoder = Dense(input_dim)(encoded)\n",
    "decoder = prellll(decoder)\n",
    "# decoder = Dense(input_dim)(encoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "\n",
    "name = \"Onlykernell1\"\n",
    "\n",
    "early_stopper = EarlyStopping(monitor=\"val_loss\",\n",
    "                                  patience=50,\n",
    "                                  verbose=True,\n",
    "                                  mode=\"auto\")\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer=adamm, \n",
    "                            loss='mean_squared_error'\n",
    "                            # metrics=['accuracy']\n",
    "                           )\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint((\"/afs/cern.ch/user/f/fsiroky/models_ae/%s.h5\" % name),\n",
    "                                                  monitor=\"val_loss\",\n",
    "                                                  verbose=False,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode=\"min\")\n",
    "testerror = AdditionalValidationSets([(X_test, X_test, 'test')])\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                            epochs=nb_epoch,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            validation_split=0.25,\n",
    "                            verbose=2,\n",
    "                            callbacks=[testerror, early_stopper, checkpoint_callback]\n",
    "                         ).history\n",
    "\n",
    "#np.save('/eos/cms/store/user/fsiroky/ae_models/%s.npy' % name, history)\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_loss.npy' % name , history['loss'])\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_valloss.npy' % name, history['val_loss'])\n",
    "np.save('/afs/cern.ch/user/f/fsiroky/models_ae/%s_testloss.npy' % name , testerror.history['test_loss'])\n",
    "#test_loss does not say much as it contains anomalous lumisections too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
